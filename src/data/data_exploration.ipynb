{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f93ad5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard as zstd\n",
    "import io\n",
    "import orjson as json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef9648",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd848812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zst_lines(path: str, max_lines: int = None, subreddit_whitelist: set[str] = None):\n",
    "    \"\"\"\n",
    "    Streams a .zst file line by line, yielding only JSON rows from whitelisted subreddits.\n",
    "    Shows a tqdm progress bar for lines read.\n",
    "    \"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    with open(path, 'rb') as fh:\n",
    "        with dctx.stream_reader(fh) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "            sub_count = 0\n",
    "            for i, line in enumerate(tqdm(text_stream, desc=f\"Reading {path}\", unit=\"lines\")):\n",
    "                if not subreddit_whitelist and max_lines and i >= max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    if subreddit_whitelist and max_lines and sub_count >= max_lines:\n",
    "                        break\n",
    "                    post = json.loads(line)\n",
    "                    sub = post.get(\"subreddit\")\n",
    "                    if subreddit_whitelist and sub not in subreddit_whitelist:\n",
    "                        continue\n",
    "                    yield post\n",
    "                    sub_count += 1\n",
    "                except (json.JSONDecodeError, UnicodeDecodeError):\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e78ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading RS_2023-01.zst: 3038352lines [00:39, 76074.91lines/s]\n"
     ]
    }
   ],
   "source": [
    "subs = {'teenagers'}\n",
    "data = list(read_zst_lines(\"RS_2023-01.zst\", max_lines=10000, subreddit_whitelist=subs))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb87dfb",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e9075290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('depression_casualuk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "204b498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for subreddit 'teenagers':\n",
      "Total posts: 10000\n",
      "Average title length: 46.7616\n",
      "Average selftext length: 59.3895\n",
      "Maximum title length: 300\n",
      "Maximum selftext length: 13534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def data_stats(df):\n",
    "    # Basic statistics per subreddit\n",
    "    subs = df['subreddit'].unique()\n",
    "\n",
    "    for subreddit in subs:\n",
    "        subset = df[df['subreddit'] == subreddit]\n",
    "        print(f\"Statistics for subreddit '{subreddit}':\")\n",
    "        print(f\"Total posts: {len(subset)}\")\n",
    "        print(f\"Average title length: {subset['title'].str.len().mean()}\")\n",
    "        print(f\"Average selftext length: {subset['selftext'].str.len().mean()}\")\n",
    "        print(f\"Maximum title length: {subset['title'].str.len().max()}\")\n",
    "        print(f\"Maximum selftext length: {subset['selftext'].str.len().max()}\")\n",
    "        print()\n",
    "data_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "04cefd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    try:\n",
    "        if text is None:\n",
    "            return pd.NA\n",
    "        if not isinstance(text, str):\n",
    "            return pd.NA\n",
    "        if '[removed]' in text or '[deleted]' in text:\n",
    "            return pd.NA\n",
    "        # text = re.sub(r'[^\\x00-\\x7F]+', '', text) # Remove non-ASCII\n",
    "        text = re.sub(r'\\p{Emoji}', '', text) # Remove emojis\n",
    "        text = text.replace('\\uFE0F', '') # Remove variation selector for emojis\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text) # Remove URLs\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text!r}\\nException: {e}\")\n",
    "\n",
    "def filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[['subreddit', 'title', 'selftext']].copy()\n",
    "    for col in ['title', 'selftext']:\n",
    "        for idx, text in df[col].items():\n",
    "            try:\n",
    "                df.at[idx, col] = preprocess_text(text)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at row {idx}, column '{col}': {text!r}\")\n",
    "    df = df.dropna(subset=['title', 'selftext'], how='all')\n",
    "    df = df.replace(pd.NA, '')\n",
    "    return df\n",
    "\n",
    "# df = filter_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6fb963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_non_english(text):\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    non_ascii = [c for c in text if ord(c) > 127]\n",
    "    # if non_ascii:\n",
    "    #     print(f\"Non-ASCII characters: {non_ascii}\")\n",
    "    return bool(non_ascii)\n",
    "\n",
    "def remove_non_english_posts(df):\n",
    "    mask = df.apply(lambda row: detect_non_english(row['title']) or detect_non_english(row['selftext']), axis=1)\n",
    "    return df[~mask]\n",
    "\n",
    "text = 'happy new year'\n",
    "# Remove the posts containing 'happy new year'\n",
    "def remove_happy_new_year_posts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    mask = df['title'].str.contains(text, case=True, na=False) & df['selftext'].isna()\n",
    "    return df[~mask]\n",
    "\n",
    "# df.to_csv('depression_casualuk_teenagers_cleaned_no_happy_new_year.csv', index=False)\n",
    "\n",
    "# print(\"Detecting non-English content in the DataFrame...\")\n",
    "# for index, row in df.iterrows():\n",
    "#     if detect_non_english(row['title']) or detect_non_english(row['selftext']):\n",
    "#         print(f\"Non-English content detected in row {index}:\")\n",
    "#         print(f\"Title: {row['title']}\")\n",
    "#         print(f\"Selftext: {row['selftext']}\")\n",
    "#         print(\"-\" * 80)\n",
    "\n",
    "# df = remove_non_english_posts(df)\n",
    "# print(\"After removing non-English posts, checking again...\")\n",
    "# for index, row in df.iterrows():\n",
    "#     if detect_non_english(row['title']) or detect_non_english(row['selftext']):\n",
    "#         print(f\"Non-English content detected in row {index}:\")\n",
    "#         print(f\"Title: {row['title']}\")\n",
    "#         print(f\"Selftext: {row['selftext']}\")\n",
    "#         print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "632fa256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for subreddit 'teenagers':\n",
      "Total posts: 9909\n",
      "Average title length: 45.987687960440006\n",
      "Average selftext length: 52.24260773034615\n",
      "Maximum title length: 300\n",
      "Maximum selftext length: 13508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics per subreddit\n",
    "subs = df['subreddit'].unique()\n",
    "\n",
    "for subreddit in subs:\n",
    "    subset = df[df['subreddit'] == subreddit]\n",
    "    print(f\"Statistics for subreddit '{subreddit}':\")\n",
    "    print(f\"Total posts: {len(subset)}\")\n",
    "    print(f\"Average title length: {subset['title'].str.len().mean()}\")\n",
    "    print(f\"Average selftext length: {subset['selftext'].str.len().mean()}\")\n",
    "    print(f\"Maximum title length: {subset['title'].str.len().max()}\")\n",
    "    print(f\"Maximum selftext length: {subset['selftext'].str.len().max()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a807da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('teenagers_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d524018",
   "metadata": {},
   "source": [
    "# Obtaining Non-Depression Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6cda47a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading RS_2023-01.zst: 36090941lines [26:38, 22578.60lines/s]\n"
     ]
    }
   ],
   "source": [
    "subs = {'Anxiety'}\n",
    "data = list(read_zst_lines(\"RS_2023-01.zst\", subreddit_whitelist=subs))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d34820fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_dataframe(df)\n",
    "df = remove_non_english_posts(df)\n",
    "df = remove_happy_new_year_posts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9c1657a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>When you are at the end, keep pushing.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Anxiety so bad I want to kms</td>\n",
       "      <td>I have had panic attacks on and off for so man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Autophobia</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Anxiety meds</td>\n",
       "      <td>Hey guys I’m an f and I want to get started on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Tingling only in feet from Anxiety?</td>\n",
       "      <td>I had a foot issue around  months ago (stabbin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8258</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Do you guys get butterflies in your chest/sens...</td>\n",
       "      <td>Especially in anticipation of something? Or ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8259</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Anxiety just before sleeping</td>\n",
       "      <td>Is it common? To me this is kind of usual and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8260</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Feeling overwhelmed while trying to take care ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8261</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Wisdom teeth and mental health ?</td>\n",
       "      <td>i just got my wisdom teeth out yesterday and h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8262</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>Diziness/lightheadedness Anxiety Symptom?</td>\n",
       "      <td>As the title suggests I am wondering if anybod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8216 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit                                              title  \\\n",
       "0      Anxiety             When you are at the end, keep pushing.   \n",
       "1      Anxiety                       Anxiety so bad I want to kms   \n",
       "2      Anxiety                                         Autophobia   \n",
       "3      Anxiety                                       Anxiety meds   \n",
       "4      Anxiety                Tingling only in feet from Anxiety?   \n",
       "...        ...                                                ...   \n",
       "8258   Anxiety  Do you guys get butterflies in your chest/sens...   \n",
       "8259   Anxiety                       Anxiety just before sleeping   \n",
       "8260   Anxiety  Feeling overwhelmed while trying to take care ...   \n",
       "8261   Anxiety                   Wisdom teeth and mental health ?   \n",
       "8262   Anxiety          Diziness/lightheadedness Anxiety Symptom?   \n",
       "\n",
       "                                               selftext  \n",
       "0                                                        \n",
       "1     I have had panic attacks on and off for so man...  \n",
       "2                                                        \n",
       "3     Hey guys I’m an f and I want to get started on...  \n",
       "4     I had a foot issue around  months ago (stabbin...  \n",
       "...                                                 ...  \n",
       "8258  Especially in anticipation of something? Or ju...  \n",
       "8259  Is it common? To me this is kind of usual and ...  \n",
       "8260                                                     \n",
       "8261  i just got my wisdom teeth out yesterday and h...  \n",
       "8262  As the title suggests I am wondering if anybod...  \n",
       "\n",
       "[8216 rows x 3 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9d3f7e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8216"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0399043",
   "metadata": {},
   "source": [
    "# Further Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b9a2ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('anxiety.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "579411d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before processing casualuk.csv: 8240 posts\n",
      "After processing casualuk.csv: 8158 posts\n",
      "Before processing suicidewatch.csv: 14418 posts\n",
      "After processing suicidewatch.csv: 14382 posts\n",
      "Before processing depression_casualuk_teenagers_no_happy_new_year.csv: 21024 posts\n",
      "After processing depression_casualuk_teenagers_no_happy_new_year.csv: 20865 posts\n",
      "Before processing adhd.csv: 13951 posts\n",
      "After processing adhd.csv: 13935 posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/yjm90tjx6td5_1v23jrh5nyc0000gn/T/ipykernel_64466/2152354859.py:12: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before processing askreddit.csv: 287138 posts\n",
      "After processing askreddit.csv: 285783 posts\n",
      "Before processing anxiety.csv: 828 posts\n",
      "After processing anxiety.csv: 815 posts\n",
      "Before processing teenagers.csv: 89529 posts\n",
      "After processing teenagers.csv: 89252 posts\n"
     ]
    }
   ],
   "source": [
    "# For each .csv in src/data\n",
    "# Remove happy new year posts\n",
    "# Code below\n",
    "\n",
    "import os\n",
    "\n",
    "for filename in os.listdir():\n",
    "    if filename == 'test.csv':\n",
    "        continue\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = filename\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Before processing {filename}: {len(df)} posts\")\n",
    "        df = remove_happy_new_year_posts(df)\n",
    "        print(f\"After processing {filename}: {len(df)} posts\")\n",
    "        df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f3c34",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4867dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.read_csv('depression_casualuk_cleaned.csv')\n",
    "df = pd.read_csv('depression_casualuk_teenagers_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bba5a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of 'depression' subreddit: 6046\n",
      "Size of 'CasualUK' subreddit: 5154\n",
      "Size of 'teenagers' subreddit: 9824\n"
     ]
    }
   ],
   "source": [
    "# Get size of depression and casualuk subreddits\n",
    "depression = df[df['subreddit'] == 'depression']\n",
    "casualuk = df[df['subreddit'] == 'CasualUK']\n",
    "teenagers = df[df['subreddit'] == 'teenagers']\n",
    "print(f\"Size of 'depression' subreddit: {len(depression)}\")\n",
    "print(f\"Size of 'CasualUK' subreddit: {len(casualuk)}\")\n",
    "print(f\"Size of 'teenagers' subreddit: {len(teenagers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77af80cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in depression titles:\n",
      "depression    503\n",
      "feel          502\n",
      "life          440\n",
      "don           409\n",
      "just          405\n",
      "want          327\n",
      "like          316\n",
      "depressed     281\n",
      "know          266\n",
      "help          222\n",
      "dtype: int64\n",
      "Top 10 words in CasualUK titles:\n",
      "just     300\n",
      "uk       233\n",
      "like     231\n",
      "new      184\n",
      "does     165\n",
      "got      150\n",
      "know     145\n",
      "today    141\n",
      "ve       123\n",
      "day      121\n",
      "dtype: int64\n",
      "Top 10 words in teenagers titles:\n",
      "like      598\n",
      "just      548\n",
      "new       432\n",
      "year      426\n",
      "guys      354\n",
      "people    352\n",
      "im        271\n",
      "don       264\n",
      "want      264\n",
      "got       245\n",
      "dtype: int64\n",
      "Top 10 words in depression selftexts:\n",
      "just      9995\n",
      "like      8143\n",
      "don       7340\n",
      "feel      7328\n",
      "life      4980\n",
      "know      4920\n",
      "want      4826\n",
      "ve        4575\n",
      "time      3686\n",
      "really    3152\n",
      "dtype: int64\n",
      "Top 10 words in CasualUK selftexts:\n",
      "just      630\n",
      "ve        493\n",
      "like      461\n",
      "know      325\n",
      "don       313\n",
      "people    304\n",
      "time      284\n",
      "got       240\n",
      "going     238\n",
      "think     210\n",
      "dtype: int64\n",
      "Top 10 words in teenagers selftexts:\n",
      "like      801\n",
      "just      685\n",
      "don       424\n",
      "know      361\n",
      "people    320\n",
      "want      309\n",
      "year      287\n",
      "time      271\n",
      "ve        268\n",
      "really    263\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Perform text analysis on data\n",
    "\n",
    "df = pd.read_csv(\"depression_casualuk_teenagers_cleaned_no_happy_new_year.csv\")\n",
    "# Replace np.nan with '' in 'selftext' column\n",
    "df['selftext'] = df['selftext'].fillna('')\n",
    "\n",
    "# Perform stopword removal, tokenization, and frequency counting\n",
    "depression = df[df['subreddit'] == 'depression']\n",
    "casualuk = df[df['subreddit'] == 'CasualUK']\n",
    "teenagers = df[df['subreddit'] == 'teenagers']\n",
    "def count_word_frequency(texts):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return word_freq.sum().sort_values(ascending=False)\n",
    "# Drop rows with NaN values in 'title'\n",
    "depression = depression.dropna(subset=['title'])\n",
    "casualuk = casualuk.dropna(subset=['title'])\n",
    "teenagers = teenagers.dropna(subset=['title'])\n",
    "\n",
    "depression_title_freq = count_word_frequency(depression['title'])\n",
    "casualuk_title_freq = count_word_frequency(casualuk['title'])\n",
    "teenagers_title_freq = count_word_frequency(teenagers['title'])\n",
    "depression_selftext_freq = count_word_frequency(depression['selftext'])\n",
    "casualuk_selftext_freq = count_word_frequency(casualuk['selftext'])\n",
    "teenagers_selftext_freq = count_word_frequency(teenagers['selftext'])\n",
    "print(\"Top 10 words in depression titles:\")\n",
    "print(depression_title_freq.head(10))\n",
    "print(\"Top 10 words in CasualUK titles:\")\n",
    "print(casualuk_title_freq.head(10))\n",
    "print(\"Top 10 words in teenagers titles:\")\n",
    "print(teenagers_title_freq.head(10))\n",
    "print(\"Top 10 words in depression selftexts:\")\n",
    "print(depression_selftext_freq.head(10))\n",
    "print(\"Top 10 words in CasualUK selftexts:\")\n",
    "print(casualuk_selftext_freq.head(10))\n",
    "print(\"Top 10 words in teenagers selftexts:\")\n",
    "print(teenagers_selftext_freq.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85b8dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics for depression titles:\n",
      "Topic 0: feel, like, life, going, lost, better, depression, point, time, day\n",
      "Topic 1: don, depression, know, want, anymore, just, need, advice, does, thoughts\n",
      "Topic 2: depressed, life, help, just, tired, hate, im, feeling, people, depression\n",
      "\n",
      "LDA Topics for depression selftexts:\n",
      "Topic 0: did, like, depression, said, life, went, took, people, pain, day\n",
      "Topic 1: just, like, don, feel, want, know, life, ve, people, time\n",
      "Topic 2: just, ve, depression, like, feel, don, time, help, work, life\n",
      "\n",
      "LDA Topics for CasualUK titles:\n",
      "Topic 0: uk, new, got, time, january, morning, like, day, just, good\n",
      "Topic 1: just, new, best, way, uk, tea, british, like, seen, late\n",
      "Topic 2: like, just, know, does, people, did, today, uk, don, ve\n",
      "\n",
      "LDA Topics for CasualUK selftexts:\n",
      "Topic 0: like, just, night, ve, going, people, think, know, gt, don\n",
      "Topic 1: just, thread, ve, like, people, time, know, don, welcome, really\n",
      "Topic 2: just, ve, like, know, don, got, time, people, work, want\n",
      "\n",
      "LDA Topics for teenagers titles:\n",
      "Topic 0: don, like, people, post, im, know, think, wanna, love, just\n",
      "Topic 1: just, got, friend, best, like, going, good, make, reddit, gt\n",
      "Topic 2: new, year, guys, like, just, years, school, help, friends, girls\n",
      "\n",
      "LDA Topics for teenagers selftexts:\n",
      "Topic 0: like, don, know, good, yeah, whoa, slatt, people, just, high\n",
      "Topic 1: like, just, don, want, people, really, know, friends, feel, year\n",
      "Topic 2: shlorp, just, like, year, time, new, don, school, know, ve\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform LDA\n",
    "\n",
    "def perform_lda(texts, n_topics=3, n_top_words=10):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features_ind = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        topics.append((topic_idx, top_features))\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# Perform LDA on depression titles and selftexts\n",
    "depression_title_topics = perform_lda(depression['title'], n_topics=3, n_top_words=10)\n",
    "depression_selftext_topics = perform_lda(depression['selftext'], n_topics=3, n_top_words=10)\n",
    "\n",
    "# Perform LDA on CasualUK titles and selftexts\n",
    "casualuk_title_topics = perform_lda(casualuk['title'], n_topics=3, n_top_words=10)\n",
    "casualuk_selftext_topics = perform_lda(casualuk['selftext'], n_topics=3, n_top_words=10)\n",
    "\n",
    "# Perform LDA on teenagers titles and selftexts\n",
    "teenagers_title_topics = perform_lda(teenagers['title'], n_topics=3, n_top_words=10)\n",
    "teenagers_selftext_topics = perform_lda(teenagers['selftext'], n_topics=3, n_top_words=10)\n",
    "\n",
    "# Print LDA topics\n",
    "def print_lda_topics(topics, subreddit, text_type):\n",
    "    print(f\"LDA Topics for {subreddit} {text_type}:\")\n",
    "    for topic_idx, top_features in topics:\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_features)}\")\n",
    "    print()\n",
    "\n",
    "print_lda_topics(depression_title_topics, 'depression', 'titles')\n",
    "print_lda_topics(depression_selftext_topics, 'depression', 'selftexts')\n",
    "print_lda_topics(casualuk_title_topics, 'CasualUK', 'titles')\n",
    "print_lda_topics(casualuk_selftext_topics, 'CasualUK', 'selftexts')\n",
    "print_lda_topics(teenagers_title_topics, 'teenagers', 'titles')\n",
    "print_lda_topics(teenagers_selftext_topics, 'teenagers', 'selftexts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2e250",
   "metadata": {},
   "source": [
    "# Check Depressive Posts in non-r/Depression Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of potentially depressive posts in r/teenagers: 185\n",
      "Number of potentially depressive posts in r/CasualUK: 47\n",
      "Number of potentially depressive posts in all subreddits: 232\n"
     ]
    }
   ],
   "source": [
    "# Check whether there are depressive posts in r/Teenagers and/or r/CasualUK\n",
    "\n",
    "def check_depressive_posts(df):\n",
    "    \"\"\"\n",
    "    Check for depressive posts in a given subreddit DataFrame.\n",
    "    Returns a DataFrame of posts that may indicate depression.\n",
    "    \"\"\"\n",
    "    keywords = ['depressed', 'depression', 'sad', 'unhappy', 'lonely', 'hopeless']\n",
    "    mask = df['title'].str.contains('|'.join(keywords), case=False, na=False) | \\\n",
    "           df['selftext'].str.contains('|'.join(keywords), case=False, na=False)\n",
    "    return df[mask]\n",
    "teenagers_depressive_posts = check_depressive_posts(teenagers)\n",
    "casualuk_depressive_posts = check_depressive_posts(casualuk)\n",
    "print(f\"Number of potentially depressive posts in r/teenagers: {len(teenagers_depressive_posts)}\")\n",
    "print(f\"Number of potentially depressive posts in r/CasualUK: {len(casualuk_depressive_posts)}\")\n",
    "print(f\"Number of potentially depressive posts in all subreddits: {len(teenagers_depressive_posts) + len(casualuk_depressive_posts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
