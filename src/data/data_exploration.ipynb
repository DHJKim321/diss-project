{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93ad5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/yjm90tjx6td5_1v23jrh5nyc0000gn/T/ipykernel_14387/4245380815.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import zstandard as zstd\n",
    "import io\n",
    "import orjson as json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef9648",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd848812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_zst_lines(path: str, max_lines: int = None, subreddit_whitelist: set[str] = None):\n",
    "    \"\"\"\n",
    "    Streams a .zst file line by line, yielding only JSON rows from whitelisted subreddits.\n",
    "    Shows a tqdm progress bar for lines read.\n",
    "    \"\"\"\n",
    "    dctx = zstd.ZstdDecompressor()\n",
    "    with open(path, 'rb') as fh:\n",
    "        with dctx.stream_reader(fh) as reader:\n",
    "            text_stream = io.TextIOWrapper(reader, encoding='utf-8')\n",
    "            sub_count = 0\n",
    "            for i, line in enumerate(tqdm(text_stream, desc=f\"Reading {path}\", unit=\"lines\")):\n",
    "                if not subreddit_whitelist and max_lines and i >= max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    if subreddit_whitelist and sub_count >= max_lines:\n",
    "                        break\n",
    "                    post = json.loads(line)\n",
    "                    sub = post.get(\"subreddit\")\n",
    "                    if subreddit_whitelist and sub not in subreddit_whitelist:\n",
    "                        continue\n",
    "                    yield post\n",
    "                    sub_count += 1\n",
    "                except (json.JSONDecodeError, UnicodeDecodeError):\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e78ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading RS_2023-01.zst: 3038352lines [00:39, 76074.91lines/s]\n"
     ]
    }
   ],
   "source": [
    "subs = {'teenagers'}\n",
    "data = list(read_zst_lines(\"RS_2023-01.zst\", max_lines=10000, subreddit_whitelist=subs))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb87dfb",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e9075290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('depression_casualuk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "204b498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for subreddit 'teenagers':\n",
      "Total posts: 10000\n",
      "Average title length: 46.7616\n",
      "Average selftext length: 59.3895\n",
      "Maximum title length: 300\n",
      "Maximum selftext length: 13534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def data_stats(df):\n",
    "    # Basic statistics per subreddit\n",
    "    subs = df['subreddit'].unique()\n",
    "\n",
    "    for subreddit in subs:\n",
    "        subset = df[df['subreddit'] == subreddit]\n",
    "        print(f\"Statistics for subreddit '{subreddit}':\")\n",
    "        print(f\"Total posts: {len(subset)}\")\n",
    "        print(f\"Average title length: {subset['title'].str.len().mean()}\")\n",
    "        print(f\"Average selftext length: {subset['selftext'].str.len().mean()}\")\n",
    "        print(f\"Maximum title length: {subset['title'].str.len().max()}\")\n",
    "        print(f\"Maximum selftext length: {subset['selftext'].str.len().max()}\")\n",
    "        print()\n",
    "data_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04cefd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    try:\n",
    "        if text is None:\n",
    "            return pd.NA\n",
    "        if not isinstance(text, str):\n",
    "            return pd.NA\n",
    "        if '[removed]' in text or '[deleted]' in text:\n",
    "            return pd.NA\n",
    "        # text = re.sub(r'[^\\x00-\\x7F]+', '', text) # Remove non-ASCII\n",
    "        text = re.sub(r'\\p{Emoji}', '', text) # Remove emojis\n",
    "        text = text.replace('\\uFE0F', '') # Remove variation selector for emojis\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text) # Remove URLs\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text!r}\\nException: {e}\")\n",
    "\n",
    "def filter_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[['subreddit', 'title', 'selftext']].copy()\n",
    "    for col in ['title', 'selftext']:\n",
    "        for idx, text in df[col].items():\n",
    "            try:\n",
    "                df.at[idx, col] = preprocess_text(text)\n",
    "            except Exception as e:\n",
    "                print(f\"Error at row {idx}, column '{col}': {text!r}\")\n",
    "    df = df.dropna(subset=['title', 'selftext'], how='all')\n",
    "    df = df.replace(pd.NA, '')\n",
    "    return df\n",
    "\n",
    "df = filter_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b6fb963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_non_english(text):\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    non_ascii = [c for c in text if ord(c) > 127]\n",
    "    # if non_ascii:\n",
    "    #     print(f\"Non-ASCII characters: {non_ascii}\")\n",
    "    return bool(non_ascii)\n",
    "\n",
    "def remove_non_english_posts(df):\n",
    "    mask = df.apply(lambda row: detect_non_english(row['title']) or detect_non_english(row['selftext']), axis=1)\n",
    "    return df[~mask]\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if detect_non_english(row['title']) or detect_non_english(row['selftext']):\n",
    "#         print(f\"Non-English content detected in row {index}:\")\n",
    "#         print(f\"Title: {row['title']}\")\n",
    "#         print(f\"Selftext: {row['selftext']}\")\n",
    "#         print(\"-\" * 80)\n",
    "\n",
    "df = remove_non_english_posts(df)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if detect_non_english(row['title']) or detect_non_english(row['selftext']):\n",
    "        print(f\"Non-English content detected in row {index}:\")\n",
    "        print(f\"Title: {row['title']}\")\n",
    "        print(f\"Selftext: {row['selftext']}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "632fa256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for subreddit 'teenagers':\n",
      "Total posts: 9909\n",
      "Average title length: 45.987687960440006\n",
      "Average selftext length: 52.24260773034615\n",
      "Maximum title length: 300\n",
      "Maximum selftext length: 13508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics per subreddit\n",
    "subs = df['subreddit'].unique()\n",
    "\n",
    "for subreddit in subs:\n",
    "    subset = df[df['subreddit'] == subreddit]\n",
    "    print(f\"Statistics for subreddit '{subreddit}':\")\n",
    "    print(f\"Total posts: {len(subset)}\")\n",
    "    print(f\"Average title length: {subset['title'].str.len().mean()}\")\n",
    "    print(f\"Average selftext length: {subset['selftext'].str.len().mean()}\")\n",
    "    print(f\"Maximum title length: {subset['title'].str.len().max()}\")\n",
    "    print(f\"Maximum selftext length: {subset['selftext'].str.len().max()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a807da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('teenagers_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f3c34",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4867dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('depression_casualuk_cleaned.csv')\n",
    "df = pd.concat([df, temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bba5a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of 'depression' subreddit: 6046\n",
      "Size of 'CasualUK' subreddit: 5167\n",
      "Size of 'teenagers' subreddit: 9909\n"
     ]
    }
   ],
   "source": [
    "# Get size of depression and casualuk subreddits\n",
    "depression = df[df['subreddit'] == 'depression']\n",
    "casualuk = df[df['subreddit'] == 'CasualUK']\n",
    "teenagers = df[df['subreddit'] == 'teenagers']\n",
    "print(f\"Size of 'depression' subreddit: {len(depression)}\")\n",
    "print(f\"Size of 'CasualUK' subreddit: {len(casualuk)}\")\n",
    "print(f\"Size of 'teenagers' subreddit: {len(teenagers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af80cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in depression titles:\n",
      "feel          502\n",
      "depression    502\n",
      "life          440\n",
      "don           409\n",
      "just          404\n",
      "want          327\n",
      "like          316\n",
      "depressed     281\n",
      "know          266\n",
      "help          222\n",
      "dtype: int64\n",
      "Top 10 words in CasualUK titles:\n",
      "january    89\n",
      "uk         80\n",
      "just       75\n",
      "new        71\n",
      "does       63\n",
      "thread     61\n",
      "today      55\n",
      "like       47\n",
      "best       43\n",
      "got        43\n",
      "dtype: int64\n",
      "Top 10 words in depression selftexts:\n",
      "just      9995\n",
      "like      8143\n",
      "don       7340\n",
      "feel      7328\n",
      "life      4980\n",
      "know      4920\n",
      "want      4826\n",
      "ve        4575\n",
      "time      3686\n",
      "really    3152\n",
      "dtype: int64\n",
      "Top 10 words in CasualUK selftexts:\n",
      "just      630\n",
      "ve        493\n",
      "like      461\n",
      "know      325\n",
      "don       313\n",
      "people    304\n",
      "time      284\n",
      "got       240\n",
      "going     238\n",
      "think     210\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Perform text analysis on data\n",
    "\n",
    "data = pd.read_csv(\"depression_casualuk_teenagers_cleaned.csv\")\n",
    "# Example text analysis: Count word frequency in titles and selftexts\n",
    "# Perform stopword removal, tokenization, and frequency counting\n",
    "depression = data[data['subreddit'] == 'depression']\n",
    "casualuk = data[data['subreddit'] == 'CasualUK']\n",
    "def count_word_frequency(texts):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return word_freq.sum().sort_values(ascending=False)\n",
    "# Drop rows with NaN values in 'title' or 'selftext'\n",
    "depression = depression.dropna(subset=['title', 'selftext'])\n",
    "casualuk = casualuk.dropna(subset=['title', 'selftext'])\n",
    "\n",
    "depression_title_freq = count_word_frequency(depression['title'])\n",
    "casualuk_title_freq = count_word_frequency(casualuk['title'])\n",
    "depression_selftext_freq = count_word_frequency(depression['selftext'])\n",
    "casualuk_selftext_freq = count_word_frequency(casualuk['selftext'])\n",
    "print(\"Top 10 words in depression titles:\")\n",
    "print(depression_title_freq.head(10))\n",
    "print(\"Top 10 words in CasualUK titles:\")\n",
    "print(casualuk_title_freq.head(10))\n",
    "print(\"Top 10 words in depression selftexts:\")\n",
    "print(depression_selftext_freq.head(10))\n",
    "print(\"Top 10 words in CasualUK selftexts:\")\n",
    "print(casualuk_selftext_freq.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "85b8dcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics for depression titles:\n",
      "Topic 0: depressed, just, depression, day, im, wish, thoughts, work, lonely, idk\n",
      "Topic 1: don, know, want, anymore, life, just, lost, time, die, depression\n",
      "Topic 2: hate, life, going, think, depression, year, ve, love, just, bad\n",
      "Topic 3: feel, like, depression, help, better, life, feeling, point, does, getting\n",
      "Topic 4: need, tired, help, life, just, depression, want, really, advice, talk\n",
      "\n",
      "LDA Topics for depression selftexts:\n",
      "Topic 0: depression, pain, day, stop, self, add, said, treatment, did, year\n",
      "Topic 1: just, like, life, time, don, know, got, ve, years, year\n",
      "Topic 2: ve, depression, just, help, feel, work, like, time, don, need\n",
      "Topic 3: just, like, feel, don, want, know, life, people, ve, time\n",
      "Topic 4: ve, just, don, like, know, today, ll, told, room, away\n",
      "\n",
      "LDA Topics for CasualUK titles:\n",
      "Topic 0: like, just, uk, thing, does, use, days, old, don, good\n",
      "Topic 1: uk, does, best, better, english, live, tea, new, help, feel\n",
      "Topic 2: thread, january, late, jan, life, old, people, know, just, british\n",
      "Topic 3: january, new, daily, observations, help, year, uk, years, today, resolution\n",
      "Topic 4: just, today, got, going, ve, good, time, night, best, house\n",
      "\n",
      "LDA Topics for CasualUK selftexts:\n",
      "Topic 0: just, like, ve, night, gt, going, know, don, getting, people\n",
      "Topic 1: just, like, ve, time, really, don, did, know, good, people\n",
      "Topic 2: just, ve, like, got, know, don, yes, week, work, going\n",
      "Topic 3: thread, ve, just, people, welcome, observations, like, today, daily, time\n",
      "Topic 4: just, like, ve, don, people, know, time, think, day, year\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform LDA\n",
    "\n",
    "def perform_lda(texts, n_topics=5, n_top_words=10):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features_ind = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        topics.append((topic_idx, top_features))\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# Perform LDA on depression titles and selftexts\n",
    "depression_title_topics = perform_lda(depression['title'], n_topics=5, n_top_words=10)\n",
    "depression_selftext_topics = perform_lda(depression['selftext'], n_topics=5, n_top_words=10)\n",
    "\n",
    "# Perform LDA on CasualUK titles and selftexts\n",
    "casualuk_title_topics = perform_lda(casualuk['title'], n_topics=5, n_top_words=10)\n",
    "casualuk_selftext_topics = perform_lda(casualuk['selftext'], n_topics=5, n_top_words=10)\n",
    "\n",
    "# Print LDA topics\n",
    "def print_lda_topics(topics, subreddit, text_type):\n",
    "    print(f\"LDA Topics for {subreddit} {text_type}:\")\n",
    "    for topic_idx, top_features in topics:\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_features)}\")\n",
    "    print()\n",
    "\n",
    "print_lda_topics(depression_title_topics, 'depression', 'titles')\n",
    "print_lda_topics(depression_selftext_topics, 'depression', 'selftexts')\n",
    "print_lda_topics(casualuk_title_topics, 'CasualUK', 'titles')\n",
    "print_lda_topics(casualuk_selftext_topics, 'CasualUK', 'selftexts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
