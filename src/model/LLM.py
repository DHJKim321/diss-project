import torch, transformers
import time

from transformers import AutoTokenizer, AutoModelForCausalLM

from prompts.templates import TEMPLATE_V1


def load_model(model_name, token):
    """
    Load a pre-trained language model and its tokenizer.

    Args:
        model_name (str): The name of the pre-trained model.
        token (str): The authentication token for accessing the model.

    Returns:
        tuple: A tuple containing the tokenizer and the model.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left', use_auth_token=token)
    pipeline = transformers.pipeline(
        "text-generation",
        model=model_name,
        tokenizer=tokenizer,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        do_sample=False,
        return_full_text=False, 
        token= token
    )
    return pipeline

def process_batch(pipeline, batch, template=TEMPLATE_V1):
    """
    Process a batch of text data using the provided language model pipeline.

    Args:
        pipeline: The pre-trained language model pipeline.
        batch (list): A list of dictionaries containing 'task_content' and 'classes'.
        template (str): The template to format the input text.

    Returns:
        list: A list of predictions generated by the model.
    """
    inputs = [template.format(task_content=item['task_content'], classes=item['classes']) for item in batch]
    start = time.time()
    print(f"Processing batch of size {len(inputs)}")
    predictions = pipeline(inputs, max_length=50, num_return_sequences=1)
    end = time.time()
    print(f"Batch processed in {end - start:.2f} seconds")
    return [pred[0]['generated_text'] for pred in predictions]
    

