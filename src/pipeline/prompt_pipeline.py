from model.LLM import *
from src.utils.data_utils import load_and_prepare_data
import torch.utils.data.dataloader as DataLoader

import os
from dotenv import load_dotenv

def run_pipeline(dataloader, pipeline):
    """
    Run the prompt pipeline to process the dataset and generate predictions.
    
    Returns:
        list: A list of predictions generated by the model.
    """
    predictions = []
    for batch in dataloader:
        batch_data = [{'task_content': item[0], 'classes': item[1]} for item in batch]
        batch_predictions = process_batch(pipeline, batch_data)
        predictions.extend(batch_predictions)
    
    return predictions

if __name__ == "__main__":
    load_dotenv()

    model_path = os.getenv("LLM_PATH")
    token_path = os.getenv("TOKEN_PATH")
    data_path = os.getenv("DATA_PATH")

    data = load_and_prepare_data(data_path)
    dataloader = DataLoader.DataLoader(data, batch_size=32, shuffle=False)

    pipeline = load_model(model_path, token_path)

    predictions = run_pipeline(dataloader, pipeline)
    